Algorithm Jurijus Pacalovas

The PAQJP_3 Compression System implements a lossless compression framework that combines multiple transformation algorithms with compression methods (PAQ, zlib, and Huffman coding) to achieve optimal compression ratios. Below, I explain each algorithm (transformations and compression methods) used in the provided code, focusing on their purpose, mechanism, and role in the system. The transformations are applied before compression to make the data more compressible, and the system automatically selects the best combination based on the smallest compressed output size. Transformations Transformations preprocess the input data to increase redundancy or patterns, making it easier for compression algorithms to achieve better ratios. Each transformation is reversible (lossless) and paired with a corresponding reverse transformation for decompression. The transformations are identified by markers (1–6) in the compressed data. – 1. Transform_01: Prime XOR Every 3 Bytes. Purpose: Increases data redundancy by XORing bytes with values derived from prime numbers, applied every third byte. – Mechanism: – Iterates over a list of prime numbers (PRIMES, 2 to 255). – For each prime, compute an XOR value: prime if prime is 2, else max(1, ceil(prime / 2 / repeat)), where repeat = 50. – For each repetition (50 times), XOR every third byte (indices 0, 3, 6, ...) with the XOR value. – Example: For prime=7, the XOR value might be ceil(7/2/50)=1. The byte at index 0 is XORed with 1, the byte at index 3 with 1, etc. – Why It Works: XORing with prime-derived values introduces predictable patterns, especially in data with low entropy, making it more compressible by algorithms like PAQ or zlib. – Reverse Transformation: The same function (transform_01) is self-inverse because XOR is its inverse (a XOR b XOR b = a). – Marker: 2 – Use Case: Effective for data with regular patterns, such as text or structured binary files. – 2. Transform_03: Pattern Chunk XOR Purpose: Inverts bits in fixed-size chunks to create patterns that compression algorithms can exploit. – Mechanism: – Divides the input data into chunks of size chunk_size=4 bytes. – For each chunk, XORs each byte with 0xFF (inverts all bits, e.g., 0x3A becomes 0xC5). – Example: Input bytes [0x01, 0x02, 0x03, 0x04] become [0xFE, 0xFD, 0xFC, 0xFB]. – Why It Works: Bit inversion creates a predictable transformation that can increase compressibility in data with certain bit patterns, especially when combined with zlib or PAQ. – Reverse Transformation: The same function (transform_03) is self-inverse because XORing with 0xFF twice restores the original byte. – Marker: 3 – Use Case: Useful for binary data or images where bit inversion can highlight repetitive structures. – 3. Transform_04: Position-Based Subtraction Purpose: Modifies bytes based on their position to create arithmetic patterns that enhance compressibility. – Mechanism: – For each byte at index i, subtract i%256 (position modulo 256) and take the result modulo 256. – Repeated repeat=50 times. – Example: For byte 0x50 at index 3, one iteration yields (0x50 - 3) % 256 = 0x4D. After 50 iterations, the effect accumulates. – Why It Works: The position-based modification introduces a smooth, predictable sequence that compression algorithms can model efficiently, especially PAQ, which excels at arithmetic patterns. – Reverse Transformation: Adds i%256 back to each byte, repeated 50 times, to reverse the subtraction (reverse_transform_04). – Marker: 1 – Use Case: Effective for data with low variability, such as logs or numerical data. – 4. Transform_05: Bit Rotation Purpose: Rotates bits within each byte to redistribute bit patterns, potentially creating more compressible sequences. – Mechanism: – Each byte’s bits are rotated left by shift=3 positions. – Uses bitwise operations: (byte << shift) | (byte >> (8 - shift)) masked with 0xFF. – Example: Byte 0x3A (binary 00111010) rotated left by 3 becomes 11010001 (0xD1). – Why It Works: Bit rotation can align similar bit patterns across bytes, improving compression for data with repetitive bit sequences, especially with zlib. – Reverse Transformation: Rotates right by 3 positions (byte >> 3 | byte << (8 - 3)), implemented in reverse_transform_05. – Marker: 5 – Use Case: Suitable for binary data or executables where bit-level patterns are common. – 5. Transform_06: Prime-Based Substitution Purpose: Substitutes each byte with another using a randomized mapping based on a seed, aiming to create patterns that compression algorithms can exploit. – Mechanism: – Generates a permutation of 0–255 using random. Shuffle with a fixed seed (seed=42). – Each byte is replaced by its corresponding value in the permutation table. – Example: If the permutation maps 0x3A to 0x7F, byte 0x3A becomes 0x7F. – Why It Works: The fixed permutation can transform data into a form with more predictable transitions, which PAQ can model effectively. – Reverse Transformation: Uses the inverse permutation (computed during initialisation) to map bytes back to their original values (reverse_transform_06). – Marker: 6 – Use Case: Effective for data with irregular byte distributions, such as compressed or encrypted files. Compression Methods After applying a transformation, the system compresses the transformed data using one of three methods, selecting the one that produces the smallest output. Each method is suited to different data characteristics. – 1. PAQ Compression Purpose: Provides high compression ratios for data with complex patterns using context modelling. – Mechanism: – Uses the paq. Compress function (assumed to be a PAQ-based library). – PAQ is a context-mixing compressor that predicts each bit based on multiple contexts (previous bytes, bits, or patterns) and uses arithmetic coding to encode predictions. – Maintains a state table (StateTable) to track context transitions, implemented in the next function. – Why It Works: PAQ excels at modelling long-range dependencies and complex patterns, making it ideal for transformed data with introduced redundancies. – Decompression: Uses paq. Decompress, which reverses the arithmetic coding and context modelling to recover the exact input. – Use Case: Best for large files with repetitive or predictable patterns after transformation, such as text or structured data. – Limitations: Slower and more memory-intensive than zlib or Huffman. – 2. Zlib Compression Purpose: Offers fast, general-purpose compression using the DEFLATE algorithm. – Mechanism: – Uses zlib. Compress, which combines LZ77 (dictionary-based matching) and Huffman coding. – LZ77 replaces repeated sequences with references to earlier occurrences. – Huffman coding assigns shorter codes to frequent symbols. – Why It Works: Zlib is efficient for a wide range of data, especially when transformations create short repeated sequences or skewed byte distributions. – Decompression: Uses zlib. Decompress to reverse the DEFLATE process, restoring the original data. – Use Case: Suitable for medium to large files where speed is important, such as images or archives. – Limitations: Less effective than PAQ for highly complex patterns. – 3. Huffman CodingPurpose: Provides bit-level compression for small files by assigning variable-length codes to symbols based on frequency. – Mechanism: – Converts the input data to a binary string (each byte to 8 bits). – Calculates bit frequencies (calculate_frequencies). – Builds a Huffman tree (build_huffman_tree) using a priority queue, where less frequent symbols get longer codes. – Generates Huffman codes (generate_huffman_codes) by traversing the tree (e.g., left=0, right=1). – Encodes the binary string using these codes (compress_data_huffman). – Converts the compressed bit string to bytes for storage. – Why It Works: Huffman coding is optimal for small data with uneven symbol frequencies, minimising the encoded size. – Decompression: – Rebuilds the Huffman tree from the compressed data’s bit frequencies. – Decodes the bit string by traversing the tree based on input bits (decompress_data_huffman). – Converts the binary string back to bytes. – Use Case: Used for files smaller than HUFFMAN_THRESHOLD (1024 bytes), such as small text files or metadata. – Limitations: Less effective for large files due to overhead and lack of context modelling.

System WorkflowCompression (compress_with_best_method):
Applies each transformation (01, 03, 04, 05, 06) to the input data.
Compresses each transformed result using PAQ and zlib.
For files < 1024 bytes, it also tries Huffman coding on the original data.
Selects the combination (transformation + compression) with the smallest output size.
Prepends a marker byte (1–6) to indicate the chosen transformation and method.
Returns the marker + compressed data.
Decompression (decompress_with_best_method):
Reads the marker byte to determine the transformation and compression method.
For marker 4 (Huffman), decode the Huffman-encoded bit string and convert it back to bytes.
For other markers (1, 2, 3, 5, 6):
Attempts PAQ decompression first.
Falls back to zlib if PAQ fails.
Applies the corresponding reverse transformation.
Returns the decompressed data. Why Lossless? All transformations and compression methods are lossless: – Transformations: Each transformation (01, 03, 04, 05, 06) is mathematically reversible, ensuring the original data can be restored exactly. – Compression Methods: – PAQ uses arithmetic coding, which is lossless. – Zlib’s DEFLATE algorithm is lossless. – Huffman coding assigns unique codes to each symbol, ensuring perfect reconstruction. – The system preserves the exact input data through the compression-decompression cycle. – Additional NotesState Table: The StateTable and next function support PAQ’s context modelling by defining state transitions for bit predictions, enhancing compression efficiency. – Prime Numbers: Used in transform_01 and find_nearest_prime_around to introduce a mathematical structure, though the latter is not used in the streamlined code. – Seed Tables: The generate_seed_tables function supports randomisation in transform_06, ensuring consistent substitutions across compression and decompression. – Performance: – PAQ is slow but achieves the best ratios for complex data. – Zlib is faster and suitable for general use. – Huffman is lightweight for small files. – Automatic Selection: The system’s strength lies in trying multiple combinations and selecting the best, adapting to the input data’s characteristics. Example For a text file with repetitive content:
Transform_04 might create arithmetic sequences that PAQ compresses well.
Transform_05 could align bits for better zlib performance.
If the file is small, Huffman coding might outperform both.

Squash minus 2**n positive numbers 0-4095 and 24-10 of dates
